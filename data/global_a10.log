2026-02-22 17:49:46 [INFO] {"timestamp": "2026-02-22 17:49:46", "message": "Initializing trajectories", "world_size": 1, "stage": "init"}
2026-02-22 17:49:46 [INFO] {"timestamp": "2026-02-22 17:49:46", "message": "Saved trajectories", "rank": 0, "num_trajectories": 10, "output_path": "output/PII redaction/trajectories/run_1/trajectories.pkl", "elapsed": 0.0004334449768066406, "stage": "save_trajectories"}
2026-02-22 17:49:46 [INFO] {"timestamp": "2026-02-22 17:49:46", "message": "Starting trajectory generation...", "rank": 0}
2026-02-22 17:49:46 [INFO] {"timestamp": "2026-02-22 17:49:46", "message": "Generating model output from old policy model", "rank": 0}
2026-02-22 17:49:46 [INFO] {"timestamp": "2026-02-22 17:49:46", "message": "Initializing RlModel", "rank": 0, "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", "device": "cuda:0", "ddp": false, "level": "macro"}
2026-02-22 17:50:00 [INFO] {"timestamp": "2026-02-22 17:50:00", "message": "Sample 0 took 9.12s", "rank": 0}
2026-02-22 17:51:10 [INFO] {"timestamp": "2026-02-22 17:51:10", "message": "Finished generating 10 samples in 79.73s", "rank": 0}
2026-02-22 17:51:11 [INFO] {"timestamp": "2026-02-22 17:51:11", "message": "Generate Old Policy Log Probs for trajectories", "rank": 0}
2026-02-22 17:51:11 [INFO] {"timestamp": "2026-02-22 17:51:11", "message": "Initializing RlModel", "rank": 0, "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", "device": "cuda:0", "ddp": false, "level": "macro"}
2026-02-22 17:51:22 [INFO] {"timestamp": "2026-02-22 17:51:22", "message": "Length params ready", "stage": "length_params", "x0": 318.95, "k": 0.031526285035648666, "min_value": 100.0, "plateau_value": 537.9, "source": "computed"}
2026-02-22 17:51:22 [INFO] {"timestamp": "2026-02-22 17:51:22", "message": "Generating rewards for trajectories", "rank": 0}
2026-02-22 17:51:22 [INFO] {"timestamp": "2026-02-22 17:51:22", "message": "Generating Advantage for trajectories", "rank": 0}
2026-02-22 17:51:22 [INFO] {"timestamp": "2026-02-22 17:51:22", "message": "Generate Ref Policy Log Probs for trajectories", "rank": 0}
2026-02-22 17:51:22 [INFO] {"timestamp": "2026-02-22 17:51:22", "message": "Initializing RlModel", "rank": 0, "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", "device": "cuda:0", "ddp": false, "level": "macro"}
2026-02-22 17:51:33 [ERROR] /opt/conda/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
2026-02-22 17:51:33 [ERROR]   warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
2026-02-22 17:51:39 [ERROR] Command failed with return code 1
2026-02-22 17:51:39 [ERROR] === SUBPROCESS STDOUT ===
2026-02-22 17:51:39 [INFO] {"timestamp": "2026-02-22 17:51:37", "message": "WANDB_API_KEY not found in .env", "level": "error"}
2026-02-22 17:51:39 [ERROR] {"timestamp": "2026-02-22 17:51:37", "message": "WANDB_API_KEY not found in .env", "level": "error"}
2026-02-22 17:51:39 [ERROR] === SUBPROCESS STDERR ===
2026-02-22 17:51:39 [ERROR] /opt/conda/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
2026-02-22 17:51:39 [ERROR]   warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
2026-02-22 17:51:39 [ERROR] Traceback (most recent call last):
2026-02-22 17:51:39 [ERROR]   File "/app/scripts/training_manager.py", line 423, in <module>
2026-02-22 17:51:39 [ERROR]     run_training(config)
2026-02-22 17:51:39 [ERROR]   File "/app/scripts/training_manager.py", line 327, in run_training
2026-02-22 17:51:39 [ERROR]     rank, local_rank, world_size = init_distributed_if_needed()
2026-02-22 17:51:39 [ERROR]   File "/app/scripts/utils/dist_utils.py", line 14, in init_distributed_if_needed
2026-02-22 17:51:39 [ERROR]     dist.init_process_group(backend="nccl")
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 86, in wrapper
2026-02-22 17:51:39 [ERROR]     func_return = func(*args, **kwargs)
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1184, in init_process_group
2026-02-22 17:51:39 [ERROR]     default_pg, _ = _new_process_group_helper(
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1339, in _new_process_group_helper
2026-02-22 17:51:39 [ERROR]     backend_class = ProcessGroupNCCL(
2026-02-22 17:51:39 [ERROR] ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
2026-02-22 17:51:39 [ERROR] [2026-02-22 12:21:39,586] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 274) of binary: /opt/conda/bin/python
2026-02-22 17:51:39 [ERROR] Traceback (most recent call last):
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/bin/torchrun", line 6, in <module>
2026-02-22 17:51:39 [ERROR]     sys.exit(main())
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
2026-02-22 17:51:39 [ERROR]     return f(*args, **kwargs)
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
2026-02-22 17:51:39 [ERROR]     run(args)
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
2026-02-22 17:51:39 [ERROR]     elastic_launch(
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
2026-02-22 17:51:39 [ERROR]     return launch_agent(self._config, self._entrypoint, list(args))
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
2026-02-22 17:51:39 [ERROR]     raise ChildFailedError(
2026-02-22 17:51:39 [ERROR] torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2026-02-22 17:51:39 [ERROR] ============================================================
2026-02-22 17:51:39 [ERROR] scripts/training_manager.py FAILED
2026-02-22 17:51:39 [ERROR] ------------------------------------------------------------
2026-02-22 17:51:39 [ERROR] Failures:
2026-02-22 17:51:39 [ERROR]   <NO_OTHER_FAILURES>
2026-02-22 17:51:39 [ERROR] ------------------------------------------------------------
2026-02-22 17:51:39 [ERROR] Root Cause (first observed failure):
2026-02-22 17:51:39 [ERROR] [0]:
2026-02-22 17:51:39 [ERROR]   time      : 2026-02-22_12:21:39
2026-02-22 17:51:39 [ERROR]   host      : 590f6fb7696b
2026-02-22 17:51:39 [ERROR]   rank      : 0 (local_rank: 0)
2026-02-22 17:51:39 [ERROR]   exitcode  : 1 (pid: 274)
2026-02-22 17:51:39 [ERROR]   error_file: <N/A>
2026-02-22 17:51:39 [ERROR]   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2026-02-22 17:51:39 [ERROR] ============================================================
2026-02-22 17:51:39 [ERROR] /opt/conda/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
2026-02-22 17:51:39 [ERROR]   warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
2026-02-22 17:51:39 [ERROR] Traceback (most recent call last):
2026-02-22 17:51:39 [ERROR]   File "/app/scripts/training_manager.py", line 423, in <module>
2026-02-22 17:51:39 [ERROR]     run_training(config)
2026-02-22 17:51:39 [ERROR]   File "/app/scripts/training_manager.py", line 327, in run_training
2026-02-22 17:51:39 [ERROR]     rank, local_rank, world_size = init_distributed_if_needed()
2026-02-22 17:51:39 [ERROR]   File "/app/scripts/utils/dist_utils.py", line 14, in init_distributed_if_needed
2026-02-22 17:51:39 [ERROR]     dist.init_process_group(backend="nccl")
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 86, in wrapper
2026-02-22 17:51:39 [ERROR]     func_return = func(*args, **kwargs)
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1184, in init_process_group
2026-02-22 17:51:39 [ERROR]     default_pg, _ = _new_process_group_helper(
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1339, in _new_process_group_helper
2026-02-22 17:51:39 [ERROR]     backend_class = ProcessGroupNCCL(
2026-02-22 17:51:39 [ERROR] ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
2026-02-22 17:51:39 [ERROR] [2026-02-22 12:21:39,586] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 274) of binary: /opt/conda/bin/python
2026-02-22 17:51:39 [ERROR] Traceback (most recent call last):
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/bin/torchrun", line 6, in <module>
2026-02-22 17:51:39 [ERROR]     sys.exit(main())
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
2026-02-22 17:51:39 [ERROR]     return f(*args, **kwargs)
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
2026-02-22 17:51:39 [ERROR]     run(args)
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
2026-02-22 17:51:39 [ERROR]     elastic_launch(
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
2026-02-22 17:51:39 [ERROR]     return launch_agent(self._config, self._entrypoint, list(args))
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
2026-02-22 17:51:39 [ERROR]     raise ChildFailedError(
2026-02-22 17:51:39 [ERROR] torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2026-02-22 17:51:39 [ERROR] ============================================================
2026-02-22 17:51:39 [ERROR] scripts/training_manager.py FAILED
2026-02-22 17:51:39 [ERROR] ------------------------------------------------------------
2026-02-22 17:51:39 [ERROR] Failures:
2026-02-22 17:51:39 [ERROR]   <NO_OTHER_FAILURES>
2026-02-22 17:51:39 [ERROR] ------------------------------------------------------------
2026-02-22 17:51:39 [ERROR] Root Cause (first observed failure):
2026-02-22 17:51:39 [ERROR] [0]:
2026-02-22 17:51:39 [ERROR]   time      : 2026-02-22_12:21:39
2026-02-22 17:51:39 [ERROR]   host      : 590f6fb7696b
2026-02-22 17:51:39 [ERROR]   rank      : 0 (local_rank: 0)
2026-02-22 17:51:39 [ERROR]   exitcode  : 1 (pid: 274)
2026-02-22 17:51:39 [ERROR]   error_file: <N/A>
2026-02-22 17:51:39 [ERROR]   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2026-02-22 17:51:39 [ERROR] ============================================================
2026-02-22 17:51:39 [INFO] {"timestamp": "2026-02-22 17:51:39", "message": "Error", "error": "Command '['env', 'PYTHONPATH=.', 'CUDA_VISIBLE_DEVICES=0', 'torchrun', '--nproc_per_node=1', 'scripts/training_manager.py', '--project_name', 'PII redaction', '--trajectory_name', 'run_1', '--training_name', 'run_1', '--no_epochs', '1', '--save_every_epoch', '1', '--accumulation_steps', '1', '--model_name', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', '--max_new_tokens', '250', '--optimizer_config_lr', '0.0001', '--optimizer_config_weight_decay', '0.1', '--reward', 'entity_extraction', '--loss_config_clip_epsilon', '0.2', '--loss_config_beta', '0.001', '--loss_config_loss', 'grpo', '--task_type', 'entity_extraction', '--use_wandb', 'False']' returned non-zero exit status 1.", "stage": "run_from_config"}
2026-02-22 17:51:39 [ERROR] Traceback (most recent call last):
2026-02-22 17:51:39 [ERROR]   File "/app/run_from_config.py", line 47, in <module>
2026-02-22 17:51:39 [ERROR] main()
2026-02-22 17:51:39 [ERROR]   File "/app/run_from_config.py", line 32, in main
2026-02-22 17:51:39 [ERROR] run_group_iterations(group_iteration_config)
2026-02-22 17:51:39 [ERROR]   File "/app/run_iteration.py", line 155, in run_group_iterations
2026-02-22 17:51:39 [ERROR] run_iteration(config.iteration)
2026-02-22 17:51:39 [ERROR]   File "/app/run_iteration.py", line 112, in run_iteration
2026-02-22 17:51:39 [ERROR] _run_training(train_config)
2026-02-22 17:51:39 [ERROR]   File "/app/run_iteration.py", line 66, in _run_training
2026-02-22 17:51:39 [ERROR] run_subprocess(command)
2026-02-22 17:51:39 [ERROR]   File "/app/scripts/utils/subprocess_utils.py", line 52, in run_subprocess
2026-02-22 17:51:39 [ERROR] result = subprocess.run(command, check=check, capture_output=True, text=True)
2026-02-22 17:51:39 [ERROR]   File "/opt/conda/lib/python3.10/subprocess.py", line 526, in run
2026-02-22 17:51:39 [ERROR] raise CalledProcessError(retcode, process.args,
2026-02-22 17:51:39 [ERROR] subprocess
2026-02-22 17:51:39 [ERROR] .
2026-02-22 17:51:39 [ERROR] CalledProcessError
2026-02-22 17:51:39 [ERROR] :
2026-02-22 17:51:39 [ERROR] Command '['env', 'PYTHONPATH=.', 'CUDA_VISIBLE_DEVICES=0', 'torchrun', '--nproc_per_node=1', 'scripts/training_manager.py', '--project_name', 'PII redaction', '--trajectory_name', 'run_1', '--training_name', 'run_1', '--no_epochs', '1', '--save_every_epoch', '1', '--accumulation_steps', '1', '--model_name', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', '--max_new_tokens', '250', '--optimizer_config_lr', '0.0001', '--optimizer_config_weight_decay', '0.1', '--reward', 'entity_extraction', '--loss_config_clip_epsilon', '0.2', '--loss_config_beta', '0.001', '--loss_config_loss', 'grpo', '--task_type', 'entity_extraction', '--use_wandb', 'False']' returned non-zero exit status 1.
